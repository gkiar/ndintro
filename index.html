<!DOCTYPE html>
<html>
  <head>
    <link href="https://file.myfontastic.com/n6vo44Re5QaWo8oCKShBs7/icons.css" rel="stylesheet">
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- bower:css -->
    <!-- <link rel="stylesheet" href="/content/themes/jhu_id/bower_components/normalize-css/normalize.css"> -->
    <link rel="stylesheet" href="fonts/gentona/gentona.css">
    <link rel="stylesheet" href="fonts/titling-gothic/titling-gothic.css">
    <link rel="stylesheet" href="fonts/quadon/quadon.css">
    <link rel="stylesheet" href="fonts/arnhem/arnhem.css">
    <!-- endbower -->

    <title>NeuroData Intro</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <style type="text/css">
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
      @import url(http://fonts.googleapis.com/css?family=Varela+Round:regular,italic,bold,bolditalic);
      @import url(http://fonts.googleapis.com/css?family=Raleway:regular,italic,bold,bolditalic);

      body {
        font-family: 'gentona'; /* Varela Round */
      }
      /*      h1, h2, h3, h4, h5, h6 {
        font-family: 'quadon';
      }      */
      h1, h2, h3, h4, h5, h6 {
        font-family: 'quadon';
        font-weight: 400;
        margin-bottom: 0;
      }

      .remark-slide-content h1 { font-size: 3em; }
      .remark-slide-content h2 { font-size: 2em; }
      .remark-slide-content h3 { font-size: 1.6em; }
      .remark-slide-content h4 { font-size: 1.4em; }
      .footnote {
        position: absolute;
        bottom: 3em;
      }
      .navbar {
        position: absolute;
        float: center;
        top: 0em;
        font-family: 'titling-gothic';  /* Yanone Kaffeesatz */
        font-weight: 200;
        color: #A7A7A7;
      }
      .navbar a {
        color: #A7A7A7;
      }
      .bbar {
        position: absolute;
        bottom: 13px;
        left: 13px;
        font-family: 'titling-gothic';  /* Yanone Kaffeesatz */
        font-weight: 200;
        color: #A7A7A7;
      }
      .bbar a {
        color: #A7A7A7;
      }
      .btn {
        background: #424242;
        height: 2em;
      }
      .remark-slide-content {
        font-size: 1.5em;
        background: #272822;
        color: white;
      }
      li p { line-height: 1.25em; }
      .r { color: #fa0000; }
      .y { color: #FFFF00; }
      .pink { color: #FF87F3;}
      .orange { color: #FFA500;}
      .g { color: #00CC00; }
      .blue { color: #75E9FF;}
      .purple { color: #A149A9;}
      .large { font-size: 2em; }
      .black { color: black; background-color: white;}
      a, a > code {
        color: rgb(249, 38, 114);
        text-decoration: none;
      }
      code {
        background: #424242;
        border-radius: 5px;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .remark-code-line-highlighted     { background-color: #373832; }
/*      .pull-left {
        float: left;
        width: 47%;
      }*/
      .pull-left {
        float: left;
        width: 47%;
      }
      .pull-right {
        float: right;
        width: 47%;
      }
      .center {
        margin: auto;
        width: 100%;
        padding: 10px;
      }
      .small {
        font-size: 0.8em;
      }
/*      .pull-right {
        float: right;
        width: 47%;
      }*/
      .pull-bottom {
        position: absolute;
        bottom: 0;
      }
      .bottom {
        position: absolute;
        bottom: 35px;
        left: 13px;
        font-family: 'Yanone Kaffeesatz';
      }
      .pull-right ~ p {
        clear: both;
      }
      #slideshow .slide .content code {
        font-size: 0.8em;
      }
      #slideshow .slide .content pre code {
        font-size: 0.9em;
        padding: 15px;
      }
      .inverse {
        background: white;
        color: white;
        text-shadow: 0 0 20px #333;
      }
      .inverse p {
        color: white;
      }
      .inverse h1, .inverse h2 {
        color: #f3f3f3;
        line-height: 0.8em;
      }
      #frame { zoom: 0.75; -moz-transform: scale(0.75); -moz-transform-origin: 0 0; }

      /* Slide-specific styling */
      #slide-inverse .footnote {
        bottom: 12px;
        left: 20px;
      }
      #slide-how .slides {
        font-size: 0.9em;
        position: absolute;
        top:  151px;
        right: 140px;
      }
      #slide-how .slides h3 {
        margin-top: 0.2em;
      }
      #slide-how .slides .first, #slide-how .slides .second {
        padding: 1px 20px;
        height: 90px;
        width: 120px;
        -moz-box-shadow: 0 0 10px #777;
        -webkit-box-shadow: 0 0 10px #777;
        box-shadow: 0 0 10px #777;
      }
      #slide-how .slides .first {
        background: #fff;
        position: absolute;
        top: 20%;
        left: 20%;
        z-index: 1;
      }
      #slide-how .slides .second {
        position: relative;
        background: #fff;
        z-index: 0;
      }

      /* Two-column layout */
      .left-column {
        color: #777;
        width: 20%;
        height: 92%;
        float: left;
      }
      .left-column h2:last-of-type, .left-column h3:last-child {
        color: #000;
      }
      .right-column {
        width: 75%;
        float: right;
        padding-top: 1em;
      }
      br {
        line-height: 50%;
      }
      .task {
        float: right;
        font-size: 0.9em;
        padding-top: 0.6em;
      }

    </style>
  </head>
  <body onload="var slideshow = remark.create();">
    <textarea id="source">



class: center, middle

# [NeuroData](http://neurodata.io):
## Enabling Petascale Neuroscience for Everyone

<!-- ### Joshua T. Vogelstein -->
<!-- ### {[BME](http://bme.jhu.edu),[ICM](http://icm.jhu.edu),[CIS](http://cis.jhu.edu),[Kavli](http://kndi.jhu.edu)}@[jhu](http://jhu.edu) -->

<!-- #### e: [jovo@jhu.edu](mailto:jovo@jhu.edu) | w:  -->
<!-- ### [NeuroData.io](http://neurodata.io) -->



#### these slides: <http://docs.neurodata.io/ndintro>

<br>
<!-- [prof joshua t. vogelstein](http://jovo.me) -->

{[bme](http://www.bme.jhu.edu/),[icm](http://icm.jhu.edu/),[cis](http://cis.jhu.edu/),[idies](http://idies.jhu.edu/),kavli,[cs](http://engineering.jhu.edu/computer-science/), [ams](http://engineering.jhu.edu/ams/), [neuro](http://neuroscience.jhu.edu/)}@[jhu](https://www.jhu.edu/)

<br>

.center[please ask questions: [support@neurodata.io](mailto:support at neurodata dot io)!]



---

layout: true

.bbar[ _[Intro](#intro): Opportunity_ | [Tools](#tools) | [Applications](#applications) | [Discussion](#disc)]


---
name: intro

# Big Scientific Data

<br>

.pull-left[
- molecular genomics (1D)
]
.pull-right[
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/A_genome_alignment_of_eight_Yersinia_isolates.png/380px-A_genome_alignment_of_eight_Yersinia_isolates.png" alt="Drawing" style="width: 400px;"/>]


---


# Big Scientific Data

<br>

.pull-left[
- molecular genomics (1D)
- cosmology (2D)
]

.pull-right[
<img src="https://upload.wikimedia.org/wikipedia/commons/3/3c/Ilc_9yr_moll4096.png" alt="Drawing" style="width: 400px;"/>]


---


# Big Scientific Data

<br>

.pull-left[
- molecular genomics (1D)
- cosmology (2D)
- Neuroscience! (3D+)
]

.pull-right[
<img src="http://www.nature.com/neuro/journal/v17/n11/images/nn.3839-F1.jpg
" alt="Drawing" style="width: 400px;"/>]


---

<!-- https://openwiki.janelia.org/wiki/download/attachments/34505478/Image%201%20-%20Medulla%20EM%20Stack%20-%20Final.jpg?version=1&modificationDate=1375908720000&api=v2 -->


### Opportunity: Big Neuro Data

.pull-left[
.center[
<iframe width="380" height="300" src="https://www.youtube.com/embed/dS_ONoUrptg?rel=0" frameborder="0" allowfullscreen></iframe>
]
- Modality: Transmission Electron Microscopy Camera Array
- 4 x 4 x 40 nm^3
- 450 x 350 x 50 micron^3
- Size: ~10 TB
- Cite: Bock et al. (Nature) 2011
]


--

.pull-right[
.center[
<iframe width="380" height="300" src="https://www.youtube.com/embed/1aVNRZtxeIU?rel=0" frameborder="0" allowfullscreen></iframe>
]

- Modality: Serial Electron Microscopy w/ATUM
- Resolution: 3 x 3 x 30 nm<sup>3</sup>
- Volume: 40 x 40 x 50 &mu;m<sup>3</sup>
- Size: ~1 TB
- Cite: Kasthuri et al. (Cell) 2015
]



---

layout: true

.bbar[ _[Intro](#intro): Challenge_ | [Tools](#tools) | [Applications](#applications) | [Discussion](#disc)]


---

## Challenge #1: Reproducible and Extensible Science


.center[
<iframe width="372" height="272" src="https://www.youtube.com/embed/1aVNRZtxeIU?rel=0" frameborder="0" allowfullscreen></iframe>
]


- 916 excitatory axons
- 1,036 dendritic spines
- 7,505 spine touches
- 1,037 synapses
- Question: Many different anatomical questions that require a catalog of all spatial objects in ontology for efficient search, discovery, analysis.




---

## Challenge #2: Synapse Spatial Point Process Pattern




.center[
<iframe width="484" height="272" src="https://www.youtube.com/embed/dS_ONoUrptg?rel=0" frameborder="0" allowfullscreen></iframe>
]

- 8 million mm<sup>3</sup> &#10141; 8 million synapses
- Manual labeling requires ~10 sec / synapse
- 60 sec x 60 min x 8 hrs x 250 days = 8 million seconds
- A person can find 8 million synapses working for 10 years
- Question: Are synapses distributed uniformly in space? If not, how are they distributed? Are there clusters?





---
layout: true
.bbar[ _[Intro](#intro)_  | [Store](#store) | [Explore](#explore) | [Parse](#parse) | [Analyze](#anal)]
---




## [NeuroData](http://neurodata.io): 4 steps to discovery

<br/>

.center[
<img src="https://www.lucidchart.com/publicSegments/view/9cc4763f-5a2a-44df-bc0c-540316d6d62d/image.png" alt="Drawing" style="width: 800px;"/>
]

<br />

.center[Existing solutions are unable to elegantly handle 3D+ petascale data]




---
name: store

### Store.Images: .y[spdb]

[Code](https://github.com/neurodata/ndstore) 
| [Tutorials](http://docs.neurodata.io/ndstore/sphinx/console.html) 
| [Data Model](http://docs.neurodata.io/ndstore/sphinx/datamodel.html) 
| [API](http://docs.neurodata.io/ndstore/) 
| [Manuscript](http://arxiv.org/abs/1306.3543) 
| [Docker](https://github.com/neurodata/ndstore/blob/master/setup/Dockerfile) 
| [Setup](https://github.com/neurodata/ndstore/tree/master/setup) 
| [Benchmarks](http://docs.neurodata.io/nddocs/more.html#r_store)
| [Examples](http://neurodata.io/projects)

.pull-left[
####  Storage Model
- Image Data Model to store images & associated metadata
- Space filling curve minimizes the number of accesses
- Dense multi-dimensional spatial array partitioned into cuboids
- Multi-resolution zoom pyramid
- 3rd party support: CATMAID, Viking, KNOSSOS, VAST, BigDataViewer
]

.pull-right[
<img src="https://upload.wikimedia.org/wikipedia/commons/3/3e/Moore3d-step3.png" style="width: 250px;"/>
<img src="images/reshier.png" style="width: 250px;"/>
]




---
name: ramondb

### Store.Shapes: .y[ramondb]


[Code](https://github.com/neurodata/ndstore) 
| [Data Model](http://docs.neurodata.io/nddocs/ndparse/ramon.html) 
| [API](http://docs.neurodata.io/ndstore/api/ramon_api.html) 
| [Manuscript](http://journal.frontiersin.org/article/10.3389/fninf.2015.00020/full)

.pull-left[
####  Data Model
  - ROI
  - Neuron
  - Segment
  - Skeleton
  - Synapse
  - Organelle
  - Node
]

.pull-right[
<img src="http://cs.brown.edu/people/tld/note/blog/13/07/26/figures/sejnowski_neural_activity_scales.jpg" style="width: 300px;"/>
]



---

### Store.Benchmarks

<br>

.center[
<img src="https://raw.githubusercontent.com/neurodata/ndpaper/master/analysis/figs/ndpaper-fig1.png" style="width: 700px;"/>
]


---
layout: true
.bbar[ [Intro](#intro)  | [Store](#store) | _[Explore](#explore)_ | [Parse](#parse) | [Analyze](#anal)]


---
name: explore

## Explore.Images:  .y[ndviz]

[Web App](http://ix.neurodata.io) 
| [Code](https://github.com/neurodata/NeuroDataViz) 
| [Issues](https://github.com/neurodata/NeuroDataViz/issues)

.pull-left[
#### Highlights
- Uses [Leaflet.js](http://leafletjs.com/)
- Enables pan & zoom in 3D
- Supports multispectral data (4D)
- Supports time-series data (4D)
- Works on mobile & Web
- Image & Anno. Metadata
- Collaboration tools
]

.pull-right[
### Examples
- [Volumetric annotation overlays](http://viz.neurodata.io/project/kharris15apical_subcell/xy/1/2064/2088/112/)
- [Multispectral blending](http://viz.neurodata.io/project/Ex10R55_glutamatergic_prepost/2/378/421/0/#)
- [Time-series](http://viz.neurodata.io/project/freeman14/xy/1/509/339/10/)
]


---


## Explore.Examples

<br>

.center[
<img src="images/nd_fig2_explore_gk.png" style="width: 600px;"/>
]



---
layout: true
.bbar[ [Intro](#intro)  | [Store](#store) | [Explore](#explore) | _[Parse](#parse)_ | [Analyze](#anal)]




---
name: 2d

### Parse.2D Stitching Artifact: .y[dmg]

[Web](https://github.com/mkazhdan/DMG) 
| [Code](https://github.com/mkazhdan/DMG) 
| [Manuscript](http://www.cs.jhu.edu/~misha/MyPapers/ToG10.pdf) 

<br>



- dmg = Distributed MultiGrid Poisson solver
- 2D image stitching histogram corrections
- Smoothing/Sharpening/High-Low Composting



.center[
<img src="images/A_full_section_2402.jpg" alt="Drawing" style="width: 300px;"/>
<img src="images/D_dmg_full_section_2402.jpg" alt="Drawing" style="width: 300px;"/>
]



---
name: 3d

### Parse.3D Color Correction: .y[gdf]


[Web](http://www.cs.jhu.edu/~misha/Code/GradientDomainFusion/) 
| [Manuscript](http://arxiv.org/pdf/1506.02079v1.pdf) 

<br>

- gdf = Gradient Domain Fusion
- 3D image histogram corrections
- Web-service coming soon



.center[
<img src="images/GDF2.png" alt="Drawing" style="width: 80%;"/>
]

---
name: ndparse

### Parse.Object Detection: .y[ndparse]

[Code](https://github.com/neurodata/ndparse)



<br />


- Can make usse of [LONI Pipeline](http://pipeline.bmap.ucla.edu/)
- .y[Vesicle] Detect all synapses in electron microscopy data: [Code](https://github.com/neurodata/vesicle)  | [Random Forest](http://docs.neurodata.io/vesicle/tutorials/vesiclerf.html) | [Deep Network](http://docs.neurodata.io/vesicle/tutorials/vesiclecnn.html) | [Manuscript](http://www.bmva.org/bmvc/2015/papers/paper081/paper081.pdf)
- .y[mbcd] Detect all cell's in histology data using RF: [Code](https://github.com/neurodata/ndod/tree/master/maca/packages/mbcd) | [Tutorial](http://docs.neurodata.io/nddocs/ndparse/mbcd.html)
- .y[nddl] Detect all cells in histology data using Deep Learning: [Tutorial](http://docs.neurodata.io/nddocs/ndparse/nddl.html)
- Integrations planned for qsub, slurm and aws submission



.center[
<img src="http://www.frontiersin.org/files/Articles/609/fninf-03-022/image_m/fninf-03-022-g002.jpg"    alt="Drawing" style="width: 300px;"/>
<img src="http://docs.neurodata.io/nddocs/ndparse/images/ndod/Y_manual1.png?raw=true"    alt="Drawing" style="width: 300px;"/>
]



---
layout: true
.bbar[ [Intro](#intro)  | [Store](#store) | [Explore](#explore) | [Parse](#parse) | _[Analyze](#anal)_]

---
name: ndio

### Python Wrapper: .y[ndio]

[Code](https://github.com/neurodata/ndio)
| [Docs](http://docs.neurodata.io/nddocs/ndio/)
| [Tutorials](http://docs.neurodata.io/nddocs/ndio/tutorials.html)

<br>

.pull-left[
- Package to easily interface with neurodata
- Open source toolbox provides image and metadata capabilities
- Easy to use interface for standalone or distributed computing across languages
]

.pull-right[
<img src="images/ndio.jpg" alt="Drawing" style="width: 300px;"/>
]



---
layout: true
.bbar[ [Intro](#intro)  | [Tools](#tools) | _[Applications](#applications)_ | [Discussion](#disc)]


---
name: applications


### Case Study #1: Reproducable and Extensible Big Data Neuroscience

#### Results from Kasthuri et al. (Cell) 2015

.pull-left[
[Claim 2](https://github.com/neurodata/kasthuri2015/blob/master/claims/claim2_axons_and_dendrites.ipynb)
1. How many different neurons?
1. How many dendrites?
1. What fraction are spiny?
1. How many unmyelinated axons?
1. What fraction are excitatory?
1. How many spines?
1. How many axon branches?
1. What fraction of cellular volume is neuron?
1. What faction of volume is cells?
1. Relative length of axon vs. dendrite?
]


.pull-right[
1. Claim 4: [How many mitochondria?](https://github.com/neurodata/kasthuri2015/blob/master/claims/claim4_mitochondria.ipynb)
1. Claim 5: [How many spines on red dendrite?](https://github.com/neurodata/kasthuri2015/blob/master/claims/claim5_spines.ipynb)
1. Claim 9: [How many vesicles?](https://github.com/neurodata/kasthuri2015/blob/master/claims/claim6_vesicles.ipynb)
1. [What is the graph?](https://github.com/neurodata/kasthuri2015/blob/master/claims/claim9_make_graph.ipynb)
]




---
name: r_syn

### Case Study #2: Is synapse distribution uniform in space?


<br />

.center[
<img src="images/rob2.png"    alt="Drawing" style="width: 75%; "/>
]

Detecting ~11.6 million synapses from the [bock11](http://openconnecto.me/bock11) dataset

???

timings

---
name: r_anal

### Case Study #2: Is synapse distribution uniform in space?


.pull-left[
<br />
1. find all synapses
2. plot 1% of them in 3D scatter plot
1. Mask out regions that cannot have synapses
1. Partition volume into cuboids
2. Count # of elements / cuboid
3. Normalize by volume of cuboid not masked
4. Plot normalized volume in 3D scatter plot
5. Plot marginals
6. Report p-value
]


.pull-right[
<br />
.center[
<img src="images/3D.png"    alt="Drawing" style="width: 350px; "/>
<img src="images/2Dproj.png"    alt="Drawing" style="width: 350px; "/>
]]






---
name: disc


## Open Science Contributions

<br />

- Reference EM and other datasets
- Reference annotations (crucial for training machine learning)
- Reference pipelines for operating on such data
- Web-services for data access
- Comprehensive cloud computing platform

--

<br />

#### Implications: it is now easier to

<br />

- Answer questions the require scale
- Engage complementary expertise
- Reproduce and extend results


---

## Coming soon...

<br />

- [Ophys Spike detection](https://github.com/jovo/oopsi)
- Ephys Spike Detection: [batch](https://github.com/jovo/spike-sorting), [online](https://github.com/decarlson/opass)
- [m2g](http://m2g.io/): MRI to Graphs
- IF Synapse Detection
- Ophys Cell Detection
- Cell detection for micron resolution data



---


# Related Work

<br />

- [CATMAID](http://catmaid.readthedocs.org/en/stable/)
- [Neurodata Without Borders](http://nwb.org/)
- [Keller Lab Block File Format](http://www.nature.com/nprot/journal/v10/n11/abs/nprot.2015.111.html)
- [Thunder](http://thunder-project.org/)


---


## Further Lowering the Barrier to Entry

<br />

- Put it all together
- Apply to other domains & questions
- We work together!





---

# References

<br />


1. Burns B et al. [The Open Connectome Project Data Cluster: Scalaballe Analysis and Vision for High-Throughput Neuroscience](http://arxiv.org/abs/1306.3543).  Scientific and Statistical Database Management (SSDBM), 2013.

1. Burns B, Vogelstein JT, Szalay AS. [From Cosmos to Connectomes: the Evolution of Data-Intensive Science](http://www.sciencedirect.com/science/article/pii/S0896627314007466). Neuron, 2015.


---
name: fam
layout: false
class:  center

# NeuroData Family

<br />



.center[
|   |   | |
| :--- | :--- | :--- |
| Store | | Randal Burns, Eric Perlman, Kunal Lillaney, Priya Manavalan, Alex Eusman
| Explore | | .orange[Misha Kazhdan, Alex Baden, Jordan Matelsky]
| Parse | | .y[Mike Miller, Nicholas Charon, Kwame Kutten, Greg Kiar, Eric Bridgeford, Greg Hager, Will Gray Roncal, Mark Chevillet,  Deank Kleissas, R. Jacob Vogelstein, Guillermo Sapiro, Anish Simhal, Konrad Kording, Eva Dyer]
| Analyze | | .blue[Joshua T. Vogelstein, Carey Priebe, Dan Naiman,  Tyler Tomita, Youngser Park, Cencheng Shen, Ivan Kuznetsov]
| Graphs | | .black[Da Zheng, Disa Mhembere, Vince Lyzinski, Avanti Athreya, Daniel Sussman, Shangsi Wang, Runze Tang, Minh Tang]
| Love | | .pink[yummy, family, friends, earth, universe, multiverse?]
]


---
class:   center


# Questions?

<!-- ### Funding &nbsp;&nbsp;&nbsp;&nbsp;   -->

<br />

_____

Funding


NIH: {CRCNS, BRAINI, TRA}

NSF:  BIGDATA

DARPA: {XDATA,GRAPHS,SIMPLEX}

IARPA: MICrONS

____


w: [neurodata.io](http://neurodata.io)

d: [docs.neurodata.io](http://docs.neurodata.io)

e: [support@neurodata.io](mailto:support@neurodata.io)


____


[more slides](http://docs.neurodata.io/ndintro/more.html)





    </textarea>
    <script src="https://gnab.github.io/remark/downloads/remark-latest.min.js" type="text/javascript">
    </script>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML&delayStartupUntil=configured" type="text/javascript"></script>
    <script type="text/javascript">
      var slideshow = remark.create();

      // Setup MathJax
      MathJax.Hub.Config({
          tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
          }
      });
      MathJax.Hub.Queue(function() {
          $(MathJax.Hub.getAllJax()).map(function(index, elem) {
              return(elem.SourceElement());
          }).parent().addClass('has-jax');
      });

      MathJax.Hub.Configured();

      var slideshow = remark.create({
        // Set the slideshow display ratio
        // Default: '4:3'
        // Alternatives: '16:9', ...
        ratio: '4:3',

        // Navigation options
        navigation: {
          // Enable or disable navigating using scroll
          // Default: true
          // Alternatives: false
          scroll: true,

          // Enable or disable navigation using touch
          // Default: true
          // Alternatives: false
          touch: true,

          // Enable or disable navigation using click
          // Default: false
          // Alternatives: true
          click: false
  },

  // Customize slide number label, either using a format string..
  // slideNumberFormat: 'Slide %current% of %total%',
  // .. or by using a format function
  slideNumberFormat: function (current, total) {
    return  current + ' / ' + total;
  },

  // Enable or disable counting of incremental slides in the slide counting
  countIncrementalSlides: false
});
    </script>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="js/bootstrap.min.js"></script>
  </body>
</html>
