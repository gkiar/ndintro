<!DOCTYPE html>
<html>
  <head>
    <link href="https://file.myfontastic.com/n6vo44Re5QaWo8oCKShBs7/icons.css" rel="stylesheet">
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- bower:css -->
    <!-- <link rel="stylesheet" href="/content/themes/jhu_id/bower_components/normalize-css/normalize.css"> -->
    <link rel="stylesheet" href="fonts/gentona/gentona.css">
    <link rel="stylesheet" href="fonts/titling-gothic/titling-gothic.css">
    <link rel="stylesheet" href="fonts/quadon/quadon.css">
    <link rel="stylesheet" href="fonts/arnhem/arnhem.css">
    <!-- endbower -->

    <title>NeuroData Intro</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <style type="text/css">
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);
      @import url(http://fonts.googleapis.com/css?family=Varela+Round:regular,italic,bold,bolditalic);
      @import url(http://fonts.googleapis.com/css?family=Raleway:regular,italic,bold,bolditalic);

      body {
        font-family: 'gentona'; /* Varela Round */
      }
      /*      h1, h2, h3, h4, h5, h6 {
        font-family: 'quadon';
      }      */
      h1, h2, h3, h4, h5, h6 {
        font-family: 'quadon';
        font-weight: 400;
        margin-bottom: 0;
      }

      .remark-slide-content h1 { font-size: 3em; }
      .remark-slide-content h2 { font-size: 2em; }
      .remark-slide-content h3 { font-size: 1.6em; }
      .remark-slide-content h4 { font-size: 1.4em; }
      .footnote {
        position: absolute;
        bottom: 3em;
      }
      .navbar {
        position: absolute;
        float: center;
        top: 0em;
        font-family: 'titling-gothic';  /* Yanone Kaffeesatz */
        font-weight: 200;
        color: #A7A7A7;
      }
      .navbar a {
        color: #A7A7A7;
      }
      .bbar {
        position: absolute;
        bottom: 13px;
        left: 13px;
        font-family: 'titling-gothic';  /* Yanone Kaffeesatz */
        font-weight: 200;
        color: #A7A7A7;
      }
      .bbar a {
        color: #A7A7A7;
      }
      .btn {
        background: #424242;
        height: 2em;
      }
      .remark-slide-content {
        font-size: 1.5em;
        background: #272822;
        color: white;
      }
      li p { line-height: 1.25em; }
      .r { color: #fa0000; }
      .y { color: #FFFF00; }
      .pink { color: #FF87F3;}
      .orange { color: #FFA500;}
      .g { color: #00CC00; }
      .blue { color: #75E9FF;}
      .purple { color: #A149A9;}
      .large { font-size: 2em; }
      .black { color: black; background-color: white;}
      a, a > code {
        color: rgb(249, 38, 114);
        text-decoration: none;
      }
      code {
        background: #424242;
        border-radius: 5px;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .remark-code-line-highlighted     { background-color: #373832; }
/*      .pull-left {
        float: left;
        width: 47%;
      }*/
      .pull-left {
        float: left;
        width: 47%;
      }
      .pull-right {
        float: right;
        width: 47%;
      }
      .center {
        margin: auto;
        width: 100%;
        padding: 10px;
      }
      .small {
        font-size: 0.8em;
      }
/*      .pull-right {
        float: right;
        width: 47%;
      }*/
      .pull-bottom {
        position: absolute;
        bottom: 0;
      }
      .bottom {
        position: absolute;
        bottom: 35px;
        left: 13px;
        font-family: 'Yanone Kaffeesatz';
      }
      .pull-right ~ p {
        clear: both;
      }
      #slideshow .slide .content code {
        font-size: 0.8em;
      }
      #slideshow .slide .content pre code {
        font-size: 0.9em;
        padding: 15px;
      }
      .inverse {
        background: white;
        color: white;
        text-shadow: 0 0 20px #333;
      }
      .inverse p {
        color: white;
      }
      .inverse h1, .inverse h2 {
        color: #f3f3f3;
        line-height: 0.8em;
      }
      #frame { zoom: 0.75; -moz-transform: scale(0.75); -moz-transform-origin: 0 0; }

      /* Slide-specific styling */
      #slide-inverse .footnote {
        bottom: 12px;
        left: 20px;
      }
      #slide-how .slides {
        font-size: 0.9em;
        position: absolute;
        top:  151px;
        right: 140px;
      }
      #slide-how .slides h3 {
        margin-top: 0.2em;
      }
      #slide-how .slides .first, #slide-how .slides .second {
        padding: 1px 20px;
        height: 90px;
        width: 120px;
        -moz-box-shadow: 0 0 10px #777;
        -webkit-box-shadow: 0 0 10px #777;
        box-shadow: 0 0 10px #777;
      }
      #slide-how .slides .first {
        background: #fff;
        position: absolute;
        top: 20%;
        left: 20%;
        z-index: 1;
      }
      #slide-how .slides .second {
        position: relative;
        background: #fff;
        z-index: 0;
      }

      /* Two-column layout */
      .left-column {
        color: #777;
        width: 20%;
        height: 92%;
        float: left;
      }
      .left-column h2:last-of-type, .left-column h3:last-child {
        color: #000;
      }
      .right-column {
        width: 75%;
        float: right;
        padding-top: 1em;
      }
      br {
        line-height: 50%;
      }
      .task {
        float: right;
        font-size: 0.9em;
        padding-top: 0.6em;
      }

    </style>
  </head>
  <body onload="var slideshow = remark.create();">
    <textarea id="source">



class: center, middle

# [NeuroData](http://neurodata.io):
### Enabling Terascale Neuroscience for Everyone

<!-- ### Joshua T. Vogelstein -->
<!-- ### {[BME](http://bme.jhu.edu),[ICM](http://icm.jhu.edu),[CIS](http://cis.jhu.edu),[Kavli](http://kndi.jhu.edu)}@[jhu](http://jhu.edu) -->

<!-- #### e: [jovo@jhu.edu](mailto:jovo@jhu.edu) | w:  -->
<!-- ### [NeuroData.io](http://neurodata.io) -->




<br>
<!-- [prof joshua t. vogelstein](http://jovo.me) -->


<br>

.center[
presented by, Joshua T. Vogelstein
<br>
{[bme](http://www.bme.jhu.edu/),[icm](http://icm.jhu.edu/),[cis](http://cis.jhu.edu/),[idies](http://idies.jhu.edu/),kavli,[cs](http://engineering.jhu.edu/computer-science/), [ams](http://engineering.jhu.edu/ams/), [neuro](http://neuroscience.jhu.edu/)}@[jhu](https://www.jhu.edu/)
<br>
please ask questions: [support@neurodata.io](mailto:support at neurodata dot io)!
<br>
these slides: <http://docs.neurodata.io/ndintro>

]



---
layout: true
.bbar[ _[Intro](#intro)_ | [Methods](#methods) | [Results](#applications)  | [Discussion](#disc)]

---
name: intro

# Big Scientific Data

<br>

.pull-left[
- molecular genomics (1D)
]
.pull-right[
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/A_genome_alignment_of_eight_Yersinia_isolates.png/380px-A_genome_alignment_of_eight_Yersinia_isolates.png" alt="Drawing" style="width: 400px;"/>]


---


# Big Scientific Data

<br>

.pull-left[
- molecular genomics (1D)
- cosmology (2D)
]

.pull-right[
<img src="https://upload.wikimedia.org/wikipedia/commons/3/3c/Ilc_9yr_moll4096.png" alt="Drawing" style="width: 400px;"/>]


---


# Big Scientific Data

<br>

.pull-left[
- molecular genomics (1D)
- cosmology (2D)
- Neuroscience! (3D+)
]

.pull-right[
<img src="http://www.nature.com/neuro/journal/v17/n11/images/nn.3839-F1.jpg
" alt="Drawing" style="width: 400px;"/>]


---

<!-- https://openwiki.janelia.org/wiki/download/attachments/34505478/Image%201%20-%20Medulla%20EM%20Stack%20-%20Final.jpg?version=1&modificationDate=1375908720000&api=v2 -->


### Opportunity: Big Neuro Data

.pull-left[
.center[
<iframe width="380" height="300" src="https://www.youtube.com/embed/dS_ONoUrptg?rel=0" frameborder="0" allowfullscreen></iframe>
]
- Modality: Transmission Electron Microscopy Camera Array
- 4 x 4 x 40 nm^3
- 450 x 350 x 50 micron^3
- Size: ~10 TB
- Cite: Bock et al. (Nature) 2011
]


--

.pull-right[
.center[
<iframe width="380" height="300" src="https://www.youtube.com/embed/1aVNRZtxeIU?rel=0" frameborder="0" allowfullscreen></iframe>
]

- Modality: Serial Electron Microscopy w/ATUM
- Resolution: 3 x 3 x 30 nm<sup>3</sup>
- Volume: 40 x 40 x 50 &mu;m<sup>3</sup>
- Size: ~1 TB
- Cite: Kasthuri et al. (Cell) 2015
]



---


## Challenge #1: Reproducible and Extensible Big NeuroScience


.center[
<iframe width="372" height="272" src="https://www.youtube.com/embed/1aVNRZtxeIU?rel=0" frameborder="0" allowfullscreen></iframe>
]


- 916 excitatory axons
- 1,036 dendritic spines
- 7,505 spine touches
- 1,037 synapses
- Goal: Enable generating and confirming anatomical hypothesis using simple tools, in a reproducible and extensible fashion.




---

## Challenge #2: Synapse Spatial Point Process Pattern




.center[
<iframe width="484" height="272" src="https://www.youtube.com/embed/dS_ONoUrptg?rel=0" frameborder="0" allowfullscreen></iframe>
]

- 8 million mm<sup>3</sup> &#10141; 8 million synapses
- Manual labeling requires ~10 sec / synapse
- 60 sec x 60 min x 8 hrs x 250 days / yr = 8 million seconds / yr
- A person can find 8 million synapses working for 10 years
- Goal: What is the spatial distribution of synapses? Is it uniform in 3D? If not, how are they not uniform? Are there clusters?





---



## [NeuroData](http://neurodata.io): 5 steps to discovery

<br/>
<br>

.center[
<img src="https://github.com/neurodata/ndpaper/raw/master/figure0.png" alt="Drawing" style="width: 800px;"/>
]

<br />

--

.left-pull[
- An cloud deployed ecosystem for big data neuroscience
- Currently hosts 10+ public datasets, another 20+ private datasets
- Total storage ~250 TB
- Enables universal accessibility, reproducibility, and extensibility
- Revealed novel spatial synapse patterns
- Free and open source software (FOSS), and free and open access data (FOAD)
]






---
layout: true
.bbar[ [Intro](#intro) | _[Methods](#methods)_ | [Results](#applications)  | [Discussion](#disc)]


---
name:methods

# Methods

- Store
  - [Images](#store)
  - Annotations(#ramondb)
- Explore
  - [Images](#ndviz)
- Process
  - [Histogram correction](#2d)
  - [Object detection](#ndparse)


---
name: store

### Store.Images 

.pull-left[
####  Goal
- Viz 1k x 1k @ video rate  anywhere
- Download cubes  @ "CV rate"
- Upload  faster than acquisition
- Only store data once
- Support  different "front-ends"
<br>



#### Challenge
- Images are big
- Each data access takes time
- 2 different use cases

<img src="images/reshier.png" style="width: 250px;"/>
]

.pull-right[
####  Actions
- .y[spdb]: [Code](https://github.com/neurodata/ndstore) 
| [Tutorials](http://docs.neurodata.io/ndstore/sphinx/console.html) 
| [API](http://docs.neurodata.io/ndstore/) 
| [Manuscript](http://arxiv.org/abs/1306.3543) 
| [Docker](https://github.com/neurodata/ndstore/blob/master/setup/Dockerfile) 
| [Setup](https://github.com/neurodata/ndstore/tree/master/setup) 
- [ndstore](https://github.com/neurodata/ndstore) for fast volume reads
  - Space filling curve
  - Dense cuboids
  - Multi-resolution pyramid
  - 3rd party support
- [ndtilecache](https://github.com/neurodata/ndtilecache) for fast viz
- [ndblaze](https://github.com/neurodata/ndblaze) for fast writes
<br>
<img src="https://upload.wikimedia.org/wikipedia/commons/3/3e/Moore3d-step3.png" style="width: 250px;"/>
]




---
name: ramondb

### Store.Shapes 


.pull-left[
#### Goal
- Spatial-semantic queries are fast & easy
- Rich enough ontology to facilitate questions at any scale
- Simple enough that is easy to use, reproduce

#### Challenge

- Annotations are big
- No standard ontology

]



.pull-right[
####  Action
  - .y[ramondb]:
[Code](https://github.com/neurodata/ndstore) 
| [Tutorial](http://docs.neurodata.io/nddocs/ndprocess/ramon.html) 
| [API](http://docs.neurodata.io/ndstore/api/ramon_api.html) 
| [Manuscript](http://journal.frontiersin.org/article/10.3389/fninf.2015.00020/full)
  - Designed [RAMON](http://docs.neurodata.io/nddocs/ndparse/ramon.html) ontology standard
  - Deployed [ramondb](http://docs.neurodata.io/ndstore/api/ramon_api.html)
  - Each voxel gets a label in hierachical ontology
  - Types include: ROI, Segment, Skeleton, Node, Synapse


<img src="images/fig5_vesicles.png" style="width: 300px;"/>
]

<!-- .pull-right[
<img src="http://cs.brown.edu/people/tld/note/blog/13/07/26/figures/sejnowski_neural_activity_scales.jpg" style="width: 300px;"/>
]
 -->


---

### Store

#### Results

<br>

.center[
<img src="https://raw.githubusercontent.com/neurodata/ndpaper/master/analysis/figs/ndpaper-fig1.png" style="width: 700px;"/>
]






---
layout: true
.bbar[ [Intro](#intro) | [Methods](#methods) | [Results](#applications)  | [Discussion](#disc)]

---
name: explore

## Explore.Images  



.pull-left[
#### Goals
- Enables pan & zoom in 3D
- Multispectral data (4D)
- Time-series data (5D)
- Mobile & Web
- Image & Anno. Metadata
- Collaboration tools


#### Challenge
- Maps are usually 2D
- Maps infrequently require volumetric overlays
- Incorporating volumetric metadata
]

.pull-right[
#### Action
- .y[ndviz]: [Web App](http://ix.neurodata.io) 
| [Code](https://github.com/neurodata/NeuroDataViz) 
| [Issues](https://github.com/neurodata/NeuroDataViz/issues)
- [Leaflet.js](http://leafletjs.com/) for navigating & overlaying
- [WebGL](https://www.khronos.org/webgl/) for dynamic color rendering
- Interfaces with ndstore, ndtilecache, ramonbd, and ndblaze to go fast

### Results
- [Volumetric annotation overlays](http://viz.neurodata.io/project/kharris15apical_subcell/xy/1/2064/2088/112/)
- [Multispectral blending](http://viz.neurodata.io/project/Ex10R55_glutamatergic_prepost/2/378/421/0/#)
- [Time-series](http://viz.neurodata.io/project/freeman14/xy/1/509/339/10/)
]


---


## Explore.Images

#### Results


.center[
<img src="images/nd_fig2_explore_gk.png" style="width: 600px;"/>
]



---
layout: true
.bbar[ [Intro](#intro) | [Methods](#methods) | [Results](#applications)  | [Discussion](#disc)]


---
name: 2d

### Process.Histogram Correction



.pull-left[
#### Goal
- Remove sharp boundary exposure artifacts
- Keep all other boundaries
- Run on 100 TB dataset
- Be "fast"
- Be automatic

#### Challenge
- Frequencies of signal and noise are overlapping
- Boundaries can span very large regions
- Each dataset has different histogram statistics
]

.pull-right[
#### Action
- 2D .y[dmg]: [Web](https://github.com/mkazhdan/DMG) 
| [Code](https://github.com/mkazhdan/DMG) 
| [Manuscript](http://www.cs.jhu.edu/~misha/MyPapers/ToG10.pdf) 
- 3D .y[gdf]:  [Web](http://www.cs.jhu.edu/~misha/Code/GradientDomainFusion/) 
| [Manuscript](http://arxiv.org/pdf/1506.02079v1.pdf) 
- Distributed implementations
- Web-service coming soon
]


---


### Process.Histogram Correction


#### Resolution

<br>

.center[
<img src="images/A_full_section_2402.jpg" alt="Drawing" style="width: 300px;"/>
<img src="images/D_dmg_full_section_2402.jpg" alt="Drawing" style="width: 300px;"/>
<br>
<img src="images/GDF2.png" alt="Drawing" style="width: 100%;"/>

]



---
name: ndparse

### Process.Object Detection

<br>

.pull-left[
#### Goal
- Automatically detect "semantic" objects
- Easy manual label
- Fast, inexpensive, simple training
- Scalable deploy

#### Challenge
- Algs tend to work "in memory"
- Tools are disparate
- Parallel computing  not "turn-key"


]

.pull-right[
#### Action
- .y[ndparse]: [Code](https://github.com/neurodata/ndparse)
- Protocol for manually labeling semantic objects
- Training ML using RF/CNN
- Deployable on cluster using [LONI Pipeline](http://pipeline.bmap.ucla.edu/)
- Integrations planned for qsub, slurm and aws submission
]



---

### Process.Object Detection

#### Results

<br>


.center[
<img src="images/ndp_bock_truth_crop.png"    alt="Drawing" style="width: 350px;"/>
<img src="images/ndp_bock_detect_crop_circles.png"    alt="Drawing" style="width: 350px;"/>

]



---
layout: true
.bbar[ [Intro](#intro) | [Methods](#methods) | _[Results](#applications)_  | [Discussion](#disc)]



---
name: applications

# Applications

<br>

- [Image Datasets](#r_im)
- [Annotation Datasets](#r_anno)
- [Case Study 1](#r_cs1)
- [Case Study 2](#r_cs2)


---
name: r_im

## Image Datasets

<br>

.center[
<img src="images/image_datasets.png" alt="Drawing" style="width: 700px;"/>
]

- 10+ public datasets
- EM, AT, Ophys, XCT
- 100+ teravoxels
- All "reference" datasets


---
name: r_anno

## Annotation Datasets



.center[
<img src="images/annotation_datasets.png" alt="Drawing" style="width: 700px;"/>
]

- 5 different publications with volumetric annotations
- No skeleton annotations yet
- Volumetric required for training machine vision
- Annotations cross spatiotemporal scale: nano, micro, time-series


---
name: r_cs1

### Case Study #1: Reproducible and Extensible Big Data Neuroscience

#### Results from Kasthuri et al. (Cell) 2015

<br>

.pull-left[
Statistical claims
1. [axons & dendrites](https://github.com/neurodata/kasthuri2015/blob/master/claims/claim02_axons_and_dendrites.ipynb)
1. [synapses](https://github.com/neurodata/kasthuri2015/blob/master/claims/claim3_synapses.ipynb)
1. [mitochondria](https://github.com/neurodata/kasthuri2015/blob/master/claims/claim4_mitochondria.ipynb)
1. [spines](https://github.com/neurodata/kasthuri2015/blob/master/claims/claim5_spines.ipynb)
1. [vesicles](https://github.com/neurodata/kasthuri2015/blob/master/claims/claim6_vesicles.ipynb)
1. [connectivity](https://github.com/neurodata/kasthuri2015/blob/master/claims/claim9_make_graph.ipynb)
]


.pull-right[
[axons and dendrites](https://github.com/neurodata/kasthuri2015/blob/master/claims/claim2_axons_and_dendrites.ipynb)
1. How many different neurons?
1. How many dendrites?
1. What fraction are spiny?
1. How many unmyelinated axons?
1. What fraction are excitatory?
1. How many spines?
1. How many axon branches?
1. What fraction of cellular volume is neuron?
1. What fraction of volume is cells?
1. Relative volume of axon vs. dendrite?
...
]






---
name: r_cs2

### Case Study #2: Is synapse distribution uniform in space?


<br />

.center[
<img src="images/rob2.png"    alt="Drawing" style="width: 75%; "/>
]

Detecting ~11.6 million synapses from the [bock11](http://openconnecto.me/bock11) dataset

???

timings

---
name: r_anal

### Case Study #2: Is synapse distribution uniform in space?

#### Data from Bock et al. (Nature) 2011


.pull-left[
<br />
1. find all synapses
1. Mask out regions that cannot have synapses
1. Partition volume into cuboids
2. Count # of elements / cuboid
3. Normalize by volume of cuboid not masked
4. Plot normalized density in 3D scatter plot
5. Plot marginal densities
6. Report p-value
]


.pull-right[
<br />
.center[
<img src="images/3D.png"    alt="Drawing" style="width: 350px; "/>
<img src="images/2Dproj.png"    alt="Drawing" style="width: 350px; "/>
]]





---
layout: true
.bbar[ [Intro](#intro) | [Methods](#methods) | [Results](#applications)  | _[Discussion](#disc)_]

---
name: disc


## Open Science Contributions

<br />

- Reference EM and other datasets
- Reference annotations (crucial for training machine learning)
- Reference pipelines for operating on such data
- Web-services for data access
- Comprehensive cloud computing platform

--

<br />

#### Implications: it is now easier to

<br />

- Answer questions that require scale
- Engage complementary expertise
- Reproduce and extend results


---

## Coming soon...

<br />


- [ndio](http://docs.neurodata.io/nddocs/ndio/) python API 
- More public datasets: CLARITY, ExM, XRM, MRI, Ophys, etc. 
- Web-services for all process steps, including 
  - [Volume reconstruction](http://abria.github.io/TeraStitcher/) 
  - Color correction ([dmg](https://github.com/mkazhdan/DMG)) 
 & [gdf](http://www.cs.jhu.edu/~misha/Code/GradientDomainFusion/) 
  - [Atlas registration](https://github.com/neurodata/ndreg) (ndreg)
  - [Ontological object detection](https://github.com/neurodata/ndparse) (ndparse)
  - [Ophys Spike detection](https://github.com/jovo/oopsi) (oopsi)
  - Ephys Spike Detection: [batch](https://github.com/jovo/spike-sorting), [online](https://github.com/decarlson/opass)
  - [MRI to Graphs](http://m2g.io/) (m2g)
- Web-services for all model steps, including:
  - Shapes
  - Graphs
  - Matrices 



---


# Related Work

<br />

- [CATMAID](http://catmaid.readthedocs.org/en/stable/)
- [Neurodata Without Borders](http://nwb.org/)
- [Keller Lab Block File Format](http://www.nature.com/nprot/journal/v10/n11/abs/nprot.2015.111.html)


---


## Further Lowering the Barrier to Entry

<br />

- Put it all together
- Apply to other domains & questions
- We work together!





---

# References

<br />


1. Burns B et al. [The Open Connectome Project Data Cluster: Scalaballe Analysis and Vision for High-Throughput Neuroscience](http://arxiv.org/abs/1306.3543).  Scientific and Statistical Database Management (SSDBM), 2013.

1. Burns B, Vogelstein JT, Szalay AS. [From Cosmos to Connectomes: the Evolution of Data-Intensive Science](http://www.sciencedirect.com/science/article/pii/S0896627314007466). Neuron, 2015.


---
name: fam
layout: false
class:  center

# NeuroData Family

<br />



.center[
|   |   | |
| :--- | :--- | :--- |
| Store | | Randal Burns, Eric Perlman, Kunal Lillaney, Priya Manavalan, Alex Eusman
| Explore | | .orange[Alex Baden,  Ivan Kuznetsov, David Marchette, Leo Duan, Albert Lee]
| Process | &nbsp;&nbsp;&nbsp;&nbsp; | .y[Mike Miller, Nicholas Charon, Misha Kazhdan, Jordan Matelsky, Kwame Kutten, Greg Kiar, Eric Bridgeford, Greg Hager, Will Gray Roncal, Mark Chevillet,  Dean Kleissas, R. Jacob Vogelstein, Guillermo Sapiro, Anish Simhal, Konrad Kording, Eva Dyer]
| Model | | .blue[Joshua T. Vogelstein, Carey Priebe, Dan Naiman,  Tyler Tomita, Youngser Park, Cencheng Shen, Ivan Kuznetsov]
| Graphs | | .black[Da Zheng, Disa Mhembere, Vince Lyzinski, Avanti Athreya, Daniel Sussman, Shangsi Wang, Runze Tang, Minh Tang]
| Love | | .pink[yummy, family, friends, earth, universe, multiverse?]
]




---
class:   center


# Questions?

<!-- ### Funding &nbsp;&nbsp;&nbsp;&nbsp;   -->

<br />

_____

Funding


NIH: {CRCNS, BRAINI, TRA}

NSF:  BIGDATA

DARPA: {XDATA,GRAPHS,SIMPLEX}

IARPA: MICrONS

____


w: [neurodata.io](http://neurodata.io)

d: [docs.neurodata.io](http://docs.neurodata.io)

e: [support@neurodata.io](mailto:support@neurodata.io)


____


[more slides](http://docs.neurodata.io/ndintro/more.html)





    </textarea>
    <script src="remark-latest.min.js" type="text/javascript">
    </script>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML&delayStartupUntil=configured" type="text/javascript"></script>
    <script type="text/javascript">
      var slideshow = remark.create();

      // Setup MathJax
      MathJax.Hub.Config({
          tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
          }
      });
      MathJax.Hub.Queue(function() {
          $(MathJax.Hub.getAllJax()).map(function(index, elem) {
              return(elem.SourceElement());
          }).parent().addClass('has-jax');
      });

      MathJax.Hub.Configured();

      var slideshow = remark.create({
        // Set the slideshow display ratio
        // Default: '4:3'
        // Alternatives: '16:9', ...
        ratio: '4:3',

        // Navigation options
        navigation: {
          // Enable or disable navigating using scroll
          // Default: true
          // Alternatives: false
          scroll: true,

          // Enable or disable navigation using touch
          // Default: true
          // Alternatives: false
          touch: true,

          // Enable or disable navigation using click
          // Default: false
          // Alternatives: true
          click: false, 

          // Enable or disable counting of incremental slides in the slide counting
          countIncrementalSlides: false



  },

  // Customize slide number label, either using a format string..
  // slideNumberFormat: 'Slide %current% of %total%',
  // .. or by using a format function
  slideNumberFormat: function (current, total) {
    return  current + ' / ' + total;
  },

  // Enable or disable counting of incremental slides in the slide counting
  countIncrementalSlides: false
});
    </script>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="js/bootstrap.min.js"></script>
  </body>
</html>
